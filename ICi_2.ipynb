{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICi_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHfGLcyKGj5O"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPEStgRzG496",
        "outputId": "e5ece516-1803-41c7-f30f-6dff66d92a9f"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFJlEqQJdW0D"
      },
      "source": [
        "# With the Cleaning ( Thasina's version)\n",
        "# Steps\n",
        "# Text cleaning and preparation\n",
        "# Upcase/downcase\n",
        "# Punctuation signs\n",
        "# Possessive pronouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9h18mmFGYmv",
        "outputId": "53881781-c747-4196-ca55-09a9b620ca6d"
      },
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        " \n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\",encoding= 'unicode_escape')\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        # print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop() \n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "def sentence_similarity2(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "    punctuation_signs = list(\"?:!.,;\")\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent1=[w.replace(\"\\r\", \" \") for w in sent1]\n",
        "    sent1=[w.replace(\"\\n\", \" \") for w in sent1]\n",
        "    sent1=[w.replace(\"    \", \" \") for w in sent1]\n",
        "    sent1=[w.replace('\"', '') for w in sent1]\n",
        "  \n",
        "   \n",
        "    for stop_word in stopwords:        \n",
        "        # regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "        sent1 = [w.replace(stop_word, '') for w in sent1]\n",
        "   \n",
        "    for punct_sign in punctuation_signs:\n",
        "      sent1=[w.replace(punct_sign, '') for w in sent1]\n",
        "    sent1=[w.replace('\"', '') for w in sent1]\n",
        " \n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "    sent2=[w.replace(\"\\r\", \" \")for w in sent2]\n",
        "    sent2=[w.replace(\"\\r\", \" \") for w in sent2]\n",
        "    sent2=[w.replace(\"\\n\", \" \") for w in sent2]\n",
        "    sent2=[w.replace(\"    \", \" \") for w in sent2]\n",
        "    sent2=[w.replace('\"', '') for w in sent2]\n",
        "    for punct_sign in punctuation_signs:\n",
        "      sent2=[w.replace(punct_sign, '') for w in sent2]\n",
        "    for stop_word in stopwords:\n",
        "        # regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "        sent2 = [w.replace(stop_word, '') for w in sent2]\n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity2(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    stop_words.append('to')\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "    # sentences = feature_engineering(sentences)\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "    summary=\"Summarize Text: \\n\", \". \".join(summarize_text)\n",
        "    return summary\n",
        "\n",
        "# let's begin\n",
        "import glob\n",
        "txt_files = glob.glob(\"sample_data/*.txt\")\n",
        "summaries=[]\n",
        "for file_name in txt_files:\n",
        "  print(file_name)\n",
        "  summary=generate_summary(file_name, 2)\n",
        "  summaries.append(summary)\n",
        "for file_name in txt_files:\n",
        "  print(file_name)\n",
        "  generate_summary(file_name, 2)\n",
        "  summaries.append(summary)\n",
        "file = open('output_1.txt', 'w')\n",
        "file.write(str(summaries))\n",
        "file.close()\n"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data/3.txt\n",
            "Indexes of top ranked_sentence order are  [(0.18021384181725142, ['Whether', 'you', 'are', 'looking', 'to', 'buy', 'a', 'car', 'and', 'need', 'to', 'protect', 'your', 'investment', 'or', 'you', 'want', 'to', 'see', 'if', 'you', 'can', 'lower', 'your', 'premiums', 'by', 'switching', 'to', 'a', 'different', 'plan,', 'we', 'have', 'you', 'covered!', 'Our', 'in-depth', 'articles', 'examine', 'and', 'dissect', 'pertinent', 'information', 'ranging', 'from', 'coverage', 'types', 'such', 'as', 'comprehensive', 'or', 'liability', 'to', 'the', 'claims', 'process']), (0.17673910319196864, ['You', 'will', 'never', 'have', 'to', 'search', 'high', 'and', 'low', 'for', 'the', 'best', 'rate', 'or', 'waste', 'time', 'filling', 'out', 'complicated', 'paperwork']), (0.17362294080123675, ['We', 'value', 'your', 'time', 'and', 'strategically', 'designed', 'our', 'site', 'to', 'be', 'easy', 'to', 'navigate', 'with', 'minimal', 'effort']), (0.17235255514973483, ['It', \"doesn't\", 'matter', 'what', 'kind', 'of', 'car', 'or', 'truck', 'you', 'drive', '-', 'we', 'have', 'partnered', 'with', 'many', 'different', 'reputable', 'insurers', 'to', 'bring', 'you', 'relevant,', 'unbiased', 'information', 'in', 'addition', 'to', 'unbeatable', 'rates', 'and', 'unmatched', 'customer', 'service']), (0.16185335759640299, ['You', \"won't\", 'have', 'to', 'waste', 'time', 'searching', 'for', 'applicable', 'auto', 'insurance', 'information']), (0.13521820144340518, ['Finally,', 'you', 'can', 'use', 'this', 'vital', 'information', 'to', 'receive', 'free,', 'customized', 'quotes', 'without', 'any', 'commitment', 'to', 'buy'])]\n",
            "Summarize Text: \n",
            " Whether you are looking to buy a car and need to protect your investment or you want to see if you can lower your premiums by switching to a different plan, we have you covered! Our in-depth articles examine and dissect pertinent information ranging from coverage types such as comprehensive or liability to the claims process. You will never have to search high and low for the best rate or waste time filling out complicated paperwork\n",
            "sample_data/2.txt\n",
            "Indexes of top ranked_sentence order are  [(0.15432993280914084, ['You', 'can', 'see', 'they', 'really', 'spent', 'some', 'time', 'bringing', 'the', 'level', 'of', 'class', 'up', 'on', 'the', 'inside', 'and', \"it's\", 'nice', 'when', 'you', 'get', 'in', 'the', \"driver's\", 'seat']), (0.15043302041842704, ['We', 'learn', 'these', 'cars', 'from', 'bumper-to-bumper,', 'day', 'in', 'and', 'day', 'out', 'so', 'we', 'can', 'truly', 'develop', 'well-crafted', 'and', 'thought-out', 'parts']), (0.1496873791322755, ['With', 'just', 'over', 'two', 'months', 'now', 'under', 'their', 'belt', 'with', 'the', 'latest', 'Civic,', 'we', 'wanted', 'to', 'check', 'in', 'and', 'see', 'what', 'they', 'discovered,', 'good', 'or', 'bad,', 'and', 'just', 'how', 'different', 'the', 'new', 'chassis', 'is', 'from', 'the', 'outgoing', 'version']), (0.14849551965156932, ['We', \"don't\", 'just', 'get', 'a', 'car,', 'develop', 'a', 'part,', 'and', 'move', 'on']), (0.14753182935492826, ['Vincent', 'said,', '\"Out', 'of', 'the', 'box,', 'the', 'car', 'is', 'pretty', 'solid']), (0.144411648507705, [\"I've\", 'been', 'daily', 'driving', 'the', 'car', 'for', 'the', 'past', '10', 'weeks', 'so', 'that', 'I', 'can', 'really', 'learn', 'everything', 'about', 'it']), (0.1051106701259542, [\"It's\", 'comfortable', 'and', 'drives', 'well,', 'and', 'the', 'inside', 'does', 'feel', 'more', 'luxurious'])]\n",
            "Summarize Text: \n",
            " You can see they really spent some time bringing the level of class up on the inside and it's nice when you get in the driver's seat. We learn these cars from bumper-to-bumper, day in and day out so we can truly develop well-crafted and thought-out parts\n",
            "sample_data/msft.txt\n",
            "Indexes of top ranked_sentence order are  [(0.12415513813030338, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.12185818198866186, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.12142415450903243, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.12114536467990505, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow.\"', 'The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.1155629315854366, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10908914280586987, ['Thatâ\\x80\\x99s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.10822175409179646, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.10530787791191326, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.07323545429708117, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI'])]\n",
            "Summarize Text: \n",
            " As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset\n",
            "sample_data/4.txt\n",
            "Indexes of top ranked_sentence order are  [(0.26273923473192085, ['Not', 'sure', 'how', 'much', 'it', 'will', 'cost', 'to', 'insure', 'your', 'automobile?', 'We', 'have', 'tirelessly', 'researched', 'every', 'single', 'make', 'and', 'model', 'to', 'deliver', 'you', 'exclusive', 'information', 'and', 'compelling', 'articles', 'that', 'pertain', 'to', 'protecting', 'your', 'specific', 'vehicle']), (0.2560391730391668, ['It', 'is', 'important', 'to', 'have', 'a', 'frame', 'of', 'reference', 'when', 'shopping,', 'making', 'the', 'ability', 'to', 'easily', 'compare', 'policies', 'vital']), (0.25578400826826475, ['To', 'start,', 'you', 'can', 'compare', 'the', 'average', 'cost', 'to', 'cover', 'a', 'specific', 'model', 'in', 'your', 'locality']), (0.22543758396064761, ['You', 'can', 'use', 'this', 'personalized', 'information', 'to', 'save', 'money', 'without', 'hassles'])]\n",
            "Summarize Text: \n",
            " Not sure how much it will cost to insure your automobile? We have tirelessly researched every single make and model to deliver you exclusive information and compelling articles that pertain to protecting your specific vehicle. It is important to have a frame of reference when shopping, making the ability to easily compare policies vital\n",
            "sample_data/1.txt\n",
            "Indexes of top ranked_sentence order are  [(0.2540415910665411, ['However,', 'after', 'those', 'few', 'parts', 'is', 'where', 'the', 'similarities', 'end.\"', 'Vincent', 'mentioned', 'that', 'the', 'most', 'surprising', 'aspect', 'of', 'the', 'new', 'model', 'is', 'its', 'engine']), (0.25243860151084185, ['Both', 'the', 'naturally', 'aspirated', '2.0-liter', 'and', '1.5-liter', 'turbo', 'carry', 'over,', 'but', 'when', 'they', 'removed', 'the', 'turbo', 'they', 'found', 'an', 'IHI', 'unit', 'as', 'opposed', 'to', 'the', 'previous', 'MHI', 'version']), (0.25056375249285523, ['These', 'items', 'required', 'little', 'to', 'no', 'modification', 'to', 'fit,', 'which', 'is', 'awesome', 'because', 'we', 'knew', 'we', 'could', 'make', 'those', 'killer', 'parts', 'from', 'the', '10th', 'get', 'straight', 'to', 'work', 'on', 'the', '11th']), (0.24295605492976194, ['\"Moving', 'on', 'to', 'the', 'downpipe', 'and', 'front', 'shock', 'tower', 'bar,', 'both', 'were', 'damn', 'near', 'identical', 'matchups'])]\n",
            "Summarize Text: \n",
            " However, after those few parts is where the similarities end.\" Vincent mentioned that the most surprising aspect of the new model is its engine. Both the naturally aspirated 2.0-liter and 1.5-liter turbo carry over, but when they removed the turbo they found an IHI unit as opposed to the previous MHI version\n",
            "sample_data/3.txt\n",
            "Indexes of top ranked_sentence order are  [(0.18021384181725142, ['Whether', 'you', 'are', 'looking', 'to', 'buy', 'a', 'car', 'and', 'need', 'to', 'protect', 'your', 'investment', 'or', 'you', 'want', 'to', 'see', 'if', 'you', 'can', 'lower', 'your', 'premiums', 'by', 'switching', 'to', 'a', 'different', 'plan,', 'we', 'have', 'you', 'covered!', 'Our', 'in-depth', 'articles', 'examine', 'and', 'dissect', 'pertinent', 'information', 'ranging', 'from', 'coverage', 'types', 'such', 'as', 'comprehensive', 'or', 'liability', 'to', 'the', 'claims', 'process']), (0.17673910319196864, ['You', 'will', 'never', 'have', 'to', 'search', 'high', 'and', 'low', 'for', 'the', 'best', 'rate', 'or', 'waste', 'time', 'filling', 'out', 'complicated', 'paperwork']), (0.17362294080123675, ['We', 'value', 'your', 'time', 'and', 'strategically', 'designed', 'our', 'site', 'to', 'be', 'easy', 'to', 'navigate', 'with', 'minimal', 'effort']), (0.17235255514973483, ['It', \"doesn't\", 'matter', 'what', 'kind', 'of', 'car', 'or', 'truck', 'you', 'drive', '-', 'we', 'have', 'partnered', 'with', 'many', 'different', 'reputable', 'insurers', 'to', 'bring', 'you', 'relevant,', 'unbiased', 'information', 'in', 'addition', 'to', 'unbeatable', 'rates', 'and', 'unmatched', 'customer', 'service']), (0.16185335759640299, ['You', \"won't\", 'have', 'to', 'waste', 'time', 'searching', 'for', 'applicable', 'auto', 'insurance', 'information']), (0.13521820144340518, ['Finally,', 'you', 'can', 'use', 'this', 'vital', 'information', 'to', 'receive', 'free,', 'customized', 'quotes', 'without', 'any', 'commitment', 'to', 'buy'])]\n",
            "Summarize Text: \n",
            " Whether you are looking to buy a car and need to protect your investment or you want to see if you can lower your premiums by switching to a different plan, we have you covered! Our in-depth articles examine and dissect pertinent information ranging from coverage types such as comprehensive or liability to the claims process. You will never have to search high and low for the best rate or waste time filling out complicated paperwork\n",
            "sample_data/2.txt\n",
            "Indexes of top ranked_sentence order are  [(0.15432993280914084, ['You', 'can', 'see', 'they', 'really', 'spent', 'some', 'time', 'bringing', 'the', 'level', 'of', 'class', 'up', 'on', 'the', 'inside', 'and', \"it's\", 'nice', 'when', 'you', 'get', 'in', 'the', \"driver's\", 'seat']), (0.15043302041842704, ['We', 'learn', 'these', 'cars', 'from', 'bumper-to-bumper,', 'day', 'in', 'and', 'day', 'out', 'so', 'we', 'can', 'truly', 'develop', 'well-crafted', 'and', 'thought-out', 'parts']), (0.1496873791322755, ['With', 'just', 'over', 'two', 'months', 'now', 'under', 'their', 'belt', 'with', 'the', 'latest', 'Civic,', 'we', 'wanted', 'to', 'check', 'in', 'and', 'see', 'what', 'they', 'discovered,', 'good', 'or', 'bad,', 'and', 'just', 'how', 'different', 'the', 'new', 'chassis', 'is', 'from', 'the', 'outgoing', 'version']), (0.14849551965156932, ['We', \"don't\", 'just', 'get', 'a', 'car,', 'develop', 'a', 'part,', 'and', 'move', 'on']), (0.14753182935492826, ['Vincent', 'said,', '\"Out', 'of', 'the', 'box,', 'the', 'car', 'is', 'pretty', 'solid']), (0.144411648507705, [\"I've\", 'been', 'daily', 'driving', 'the', 'car', 'for', 'the', 'past', '10', 'weeks', 'so', 'that', 'I', 'can', 'really', 'learn', 'everything', 'about', 'it']), (0.1051106701259542, [\"It's\", 'comfortable', 'and', 'drives', 'well,', 'and', 'the', 'inside', 'does', 'feel', 'more', 'luxurious'])]\n",
            "Summarize Text: \n",
            " You can see they really spent some time bringing the level of class up on the inside and it's nice when you get in the driver's seat. We learn these cars from bumper-to-bumper, day in and day out so we can truly develop well-crafted and thought-out parts\n",
            "sample_data/msft.txt\n",
            "Indexes of top ranked_sentence order are  [(0.12415513813030338, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.12185818198866186, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.12142415450903243, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.12114536467990505, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow.\"', 'The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.1155629315854366, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10908914280586987, ['Thatâ\\x80\\x99s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.10822175409179646, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.10530787791191326, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.07323545429708117, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI'])]\n",
            "Summarize Text: \n",
            " As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset\n",
            "sample_data/4.txt\n",
            "Indexes of top ranked_sentence order are  [(0.26273923473192085, ['Not', 'sure', 'how', 'much', 'it', 'will', 'cost', 'to', 'insure', 'your', 'automobile?', 'We', 'have', 'tirelessly', 'researched', 'every', 'single', 'make', 'and', 'model', 'to', 'deliver', 'you', 'exclusive', 'information', 'and', 'compelling', 'articles', 'that', 'pertain', 'to', 'protecting', 'your', 'specific', 'vehicle']), (0.2560391730391668, ['It', 'is', 'important', 'to', 'have', 'a', 'frame', 'of', 'reference', 'when', 'shopping,', 'making', 'the', 'ability', 'to', 'easily', 'compare', 'policies', 'vital']), (0.25578400826826475, ['To', 'start,', 'you', 'can', 'compare', 'the', 'average', 'cost', 'to', 'cover', 'a', 'specific', 'model', 'in', 'your', 'locality']), (0.22543758396064761, ['You', 'can', 'use', 'this', 'personalized', 'information', 'to', 'save', 'money', 'without', 'hassles'])]\n",
            "Summarize Text: \n",
            " Not sure how much it will cost to insure your automobile? We have tirelessly researched every single make and model to deliver you exclusive information and compelling articles that pertain to protecting your specific vehicle. It is important to have a frame of reference when shopping, making the ability to easily compare policies vital\n",
            "sample_data/1.txt\n",
            "Indexes of top ranked_sentence order are  [(0.2540415910665411, ['However,', 'after', 'those', 'few', 'parts', 'is', 'where', 'the', 'similarities', 'end.\"', 'Vincent', 'mentioned', 'that', 'the', 'most', 'surprising', 'aspect', 'of', 'the', 'new', 'model', 'is', 'its', 'engine']), (0.25243860151084185, ['Both', 'the', 'naturally', 'aspirated', '2.0-liter', 'and', '1.5-liter', 'turbo', 'carry', 'over,', 'but', 'when', 'they', 'removed', 'the', 'turbo', 'they', 'found', 'an', 'IHI', 'unit', 'as', 'opposed', 'to', 'the', 'previous', 'MHI', 'version']), (0.25056375249285523, ['These', 'items', 'required', 'little', 'to', 'no', 'modification', 'to', 'fit,', 'which', 'is', 'awesome', 'because', 'we', 'knew', 'we', 'could', 'make', 'those', 'killer', 'parts', 'from', 'the', '10th', 'get', 'straight', 'to', 'work', 'on', 'the', '11th']), (0.24295605492976194, ['\"Moving', 'on', 'to', 'the', 'downpipe', 'and', 'front', 'shock', 'tower', 'bar,', 'both', 'were', 'damn', 'near', 'identical', 'matchups'])]\n",
            "Summarize Text: \n",
            " However, after those few parts is where the similarities end.\" Vincent mentioned that the most surprising aspect of the new model is its engine. Both the naturally aspirated 2.0-liter and 1.5-liter turbo carry over, but when they removed the turbo they found an IHI unit as opposed to the previous MHI version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tatbtbmWftT",
        "outputId": "baf47017-c223-42b7-f124-af34fd345187"
      },
      "source": [
        "\n",
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        " \n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        # print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop() \n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    \n",
        "\n",
        "  \n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "    # sentences = feature_engineering(sentences)\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "    \n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "    summary=\"Summarize Text: \\n\", \". \".join(summarize_text)\n",
        "    return summary\n",
        "    file = open('output_2.txt', 'w')\n",
        "    file.write(\"Summarize for \"+file_name.join(summarize_text))\n",
        "    file.close()\n",
        "# let's begin\n",
        "import glob\n",
        "txt_files = glob.glob(\"sample_data/*.txt\")\n",
        "summaries=[]\n",
        "for file_name in txt_files:\n",
        "  print(file_name)\n",
        "  generate_summary(file_name, 2)\n",
        "  summaries.append(summary)\n",
        "file = open('output_2.txt', 'w')\n",
        "file.write(str(summaries))\n",
        "file.close()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data/3.txt\n",
            "Indexes of top ranked_sentence order are  [(0.2871626839587401, ['You', \"won't\", 'have', 'to', 'waste', 'time', 'searching', 'for', 'applicable', 'auto', 'insurance', 'information']), (0.15806554888662538, ['You', 'will', 'never', 'have', 'to', 'search', 'high', 'and', 'low', 'for', 'the', 'best', 'rate', 'or', 'waste', 'time', 'filling', 'out', 'complicated', 'paperwork']), (0.1547040871244052, ['Whether', 'you', 'are', 'looking', 'to', 'buy', 'a', 'car', 'and', 'need', 'to', 'protect', 'your', 'investment', 'or', 'you', 'want', 'to', 'see', 'if', 'you', 'can', 'lower', 'your', 'premiums', 'by', 'switching', 'to', 'a', 'different', 'plan,', 'we', 'have', 'you', 'covered!', 'Our', 'in-depth', 'articles', 'examine', 'and', 'dissect', 'pertinent', 'information', 'ranging', 'from', 'coverage', 'types', 'such', 'as', 'comprehensive', 'or', 'liability', 'to', 'the', 'claims', 'process']), (0.1465754055596412, ['Finally,', 'you', 'can', 'use', 'this', 'vital', 'information', 'to', 'receive', 'free,', 'customized', 'quotes', 'without', 'any', 'commitment', 'to', 'buy']), (0.13791850640866482, ['It', \"doesn't\", 'matter', 'what', 'kind', 'of', 'car', 'or', 'truck', 'you', 'drive', '-', 'we', 'have', 'partnered', 'with', 'many', 'different', 'reputable', 'insurers', 'to', 'bring', 'you', 'relevant,', 'unbiased', 'information', 'in', 'addition', 'to', 'unbeatable', 'rates', 'and', 'unmatched', 'customer', 'service']), (0.11557376806192299, ['We', 'value', 'your', 'time', 'and', 'strategically', 'designed', 'our', 'site', 'to', 'be', 'easy', 'to', 'navigate', 'with', 'minimal', 'effort'])]\n",
            "Summarize Text: \n",
            " You won't have to waste time searching for applicable auto insurance information. You will never have to search high and low for the best rate or waste time filling out complicated paperwork\n",
            "sample_data/2.txt\n",
            "Indexes of top ranked_sentence order are  [(0.2662838347705253, ['You', 'can', 'see', 'they', 'really', 'spent', 'some', 'time', 'bringing', 'the', 'level', 'of', 'class', 'up', 'on', 'the', 'inside', 'and', \"it's\", 'nice', 'when', 'you', 'get', 'in', 'the', \"driver's\", 'seat']), (0.19541373658241382, [\"I've\", 'been', 'daily', 'driving', 'the', 'car', 'for', 'the', 'past', '10', 'weeks', 'so', 'that', 'I', 'can', 'really', 'learn', 'everything', 'about', 'it']), (0.16272443697470954, ['We', \"don't\", 'just', 'get', 'a', 'car,', 'develop', 'a', 'part,', 'and', 'move', 'on']), (0.14077795248644265, ['We', 'learn', 'these', 'cars', 'from', 'bumper-to-bumper,', 'day', 'in', 'and', 'day', 'out', 'so', 'we', 'can', 'truly', 'develop', 'well-crafted', 'and', 'thought-out', 'parts']), (0.08714526550593406, ['Vincent', 'said,', '\"Out', 'of', 'the', 'box,', 'the', 'car', 'is', 'pretty', 'solid']), (0.08642449203770032, [\"It's\", 'comfortable', 'and', 'drives', 'well,', 'and', 'the', 'inside', 'does', 'feel', 'more', 'luxurious']), (0.061230281642274204, ['With', 'just', 'over', 'two', 'months', 'now', 'under', 'their', 'belt', 'with', 'the', 'latest', 'Civic,', 'we', 'wanted', 'to', 'check', 'in', 'and', 'see', 'what', 'they', 'discovered,', 'good', 'or', 'bad,', 'and', 'just', 'how', 'different', 'the', 'new', 'chassis', 'is', 'from', 'the', 'outgoing', 'version'])]\n",
            "Summarize Text: \n",
            " You can see they really spent some time bringing the level of class up on the inside and it's nice when you get in the driver's seat. I've been daily driving the car for the past 10 weeks so that I can really learn everything about it\n",
            "sample_data/msft.txt\n",
            "Indexes of top ranked_sentence order are  [(0.15083257041122708, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.13161201335715553, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', '\"With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.11403047674961148, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10721749759953525, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.10404298514456578, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.10031366655994461, ['That’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.10001137283486655, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow.\"', 'The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09916750119894319, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI']), (0.09277191614415067, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well'])]\n",
            "Summarize Text: \n",
            " Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset\n",
            "sample_data/4.txt\n",
            "Indexes of top ranked_sentence order are  [(0.3824754675739559, ['To', 'start,', 'you', 'can', 'compare', 'the', 'average', 'cost', 'to', 'cover', 'a', 'specific', 'model', 'in', 'your', 'locality']), (0.3583439100733339, ['Not', 'sure', 'how', 'much', 'it', 'will', 'cost', 'to', 'insure', 'your', 'automobile?', 'We', 'have', 'tirelessly', 'researched', 'every', 'single', 'make', 'and', 'model', 'to', 'deliver', 'you', 'exclusive', 'information', 'and', 'compelling', 'articles', 'that', 'pertain', 'to', 'protecting', 'your', 'specific', 'vehicle']), (0.14165608992666606, ['It', 'is', 'important', 'to', 'have', 'a', 'frame', 'of', 'reference', 'when', 'shopping,', 'making', 'the', 'ability', 'to', 'easily', 'compare', 'policies', 'vital']), (0.11752453242604413, ['You', 'can', 'use', 'this', 'personalized', 'information', 'to', 'save', 'money', 'without', 'hassles'])]\n",
            "Summarize Text: \n",
            " To start, you can compare the average cost to cover a specific model in your locality. Not sure how much it will cost to insure your automobile? We have tirelessly researched every single make and model to deliver you exclusive information and compelling articles that pertain to protecting your specific vehicle\n",
            "sample_data/1.txt\n",
            "Indexes of top ranked_sentence order are  [(0.4347821160949292, ['These', 'items', 'required', 'little', 'to', 'no', 'modification', 'to', 'fit,', 'which', 'is', 'awesome', 'because', 'we', 'knew', 'we', 'could', 'make', 'those', 'killer', 'parts', 'from', 'the', '10th', 'get', 'straight', 'to', 'work', 'on', 'the', '11th']), (0.4347821160949292, ['However,', 'after', 'those', 'few', 'parts', 'is', 'where', 'the', 'similarities', 'end.\"', 'Vincent', 'mentioned', 'that', 'the', 'most', 'surprising', 'aspect', 'of', 'the', 'new', 'model', 'is', 'its', 'engine']), (0.06521788390507068, ['Both', 'the', 'naturally', 'aspirated', '2.0-liter', 'and', '1.5-liter', 'turbo', 'carry', 'over,', 'but', 'when', 'they', 'removed', 'the', 'turbo', 'they', 'found', 'an', 'IHI', 'unit', 'as', 'opposed', 'to', 'the', 'previous', 'MHI', 'version']), (0.06521788390507068, ['\"Moving', 'on', 'to', 'the', 'downpipe', 'and', 'front', 'shock', 'tower', 'bar,', 'both', 'were', 'damn', 'near', 'identical', 'matchups'])]\n",
            "Summarize Text: \n",
            " These items required little to no modification to fit, which is awesome because we knew we could make those killer parts from the 10th get straight to work on the 11th. However, after those few parts is where the similarities end.\" Vincent mentioned that the most surprising aspect of the new model is its engine\n"
          ]
        }
      ]
    }
  ]
}